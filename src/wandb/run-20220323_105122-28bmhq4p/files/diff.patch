diff --git a/src/cfg/project/default.yaml b/src/cfg/project/default.yaml
index b460fe4..8a7d3ee 100644
--- a/src/cfg/project/default.yaml
+++ b/src/cfg/project/default.yaml
@@ -1,2 +1,3 @@
+name: mnist_training_test
 root_dir: ./
 save_dir: ./outputs
\ No newline at end of file
diff --git a/src/logger.py b/src/logger.py
index 077ea86..2710d8d 100644
--- a/src/logger.py
+++ b/src/logger.py
@@ -1,36 +1,42 @@
-import imp
-import torch
-import torch.nn as nn
-import torch.optim as optim
-from rich import pretty
-pretty.install()
-import tqdm
-import tqdm.rich
+import logging
+from rich.logging import RichHandler
+import wandb
 import logging
 import random
 import string
 from datetime import datetime
-import yaml, json
 import os, sys, time, copy, pickle
 from omegaconf import OmegaConf
-import wandb
-import tensorboardX
-# logger for wandb
+
 class Logger(object):
-    def __init__(self, wandb=False,tb=False,**kwargs):
-        self.exp_name = kwargs["exp_name"].replace(" ","_")
-        self.trainer_kwargs = kwargs["trainer_kwargs"]
-        self.save_dir = self.trainer_kwargs['save_dir'] if self.trainer_kwargs['save_dir'][-1] != '/' else self.trainer_kwargs['save_dir'][:-1]
-        self.loggers = {}
-        self.set_exp_path()
-        if wandb: self.init_wandb()
-        if tb: self.init_tensorboard()
-    def set_exp_path(self):
+    def __init__(self, project, exp_name,save_dir,kwargs={}):
+        # self.logger = logging.getLogger("rich")
+        self.project = project
+        self.exp_name = exp_name
+        self.save_dir = save_dir
+        self.kwargs = kwargs
+        self.init()
+        self.log_kwargs()
+        self.logger = logging.getLogger(__name__)
+        self.format = "%(message)s"
+        self.datafmt = "[%X]"
+        self.set_handlers()
+        logging.basicConfig(
+            format=self.format, 
+            datefmt=self.datafmt, 
+            handlers=self.handlers
+        )
+    def init(self):
         random_str = ''.join(random.choices(string.ascii_uppercase + string.digits, k=5))
         today = datetime.today().strftime("%Y-%m-%d")
         curr_time = datetime.today().strftime("%H-%M")
         self.exp_path = f'{self.save_dir}/{today}/{curr_time}-{self.exp_name}-{random_str}'
-        # create dir if not exists
+        self.create_log(today)
+        wandb.init(
+            project=self.project,
+            config=self.kwargs
+        )
+    def create_log(self, today):
         if not os.path.exists(self.save_dir): os.makedirs(self.save_dir)
         if not os.path.exists(f'{self.save_dir}/{today}'): os.makedirs(f'{self.save_dir}/{today}')
         if not os.path.exists(self.exp_path): 
@@ -41,36 +47,30 @@ class Logger(object):
             os.makedirs(f"{self.exp_path}/logs")
         else:
             raise Exception(f"Experiment path {self.exp_path} already exists")
+    def log_kwargs(self):
         conf = copy.deepcopy(self.kwargs)
         conf['exp_path'] = self.exp_path
         conf = OmegaConf.create(conf)
         with open(f'{self.exp_path}/kwargs.yaml', 'w') as fp: 
             OmegaConf.save(config=conf, f=fp)
-        self.paths = {
-            'exp_path': self.exp_path,
-            'exp_name': self.exp_name,
-            'save_dir': self.save_dir,
-            'kwargs': f'{self.exp_path}/kwargs.yaml',
-            'ckpts': f'{self.exp_path}/ckpts',
-            'optims': f'{self.exp_path}/optims',
-            'figs': f'{self.exp_path}/figs',
-        }
-    def init_wandb(self):
-        wandb.init(
-            # project=self.exp_name,
-            # name=self.exp_name,
-            # dir=self.exp_path,
-            # id=self.exp_name,
-            # config=self.trainer_kwargs,
-            # group=self.exp_name,
-        )
-        self.loggers['wand'] = wandb
-    def init_tensorboard(self):
-        self.loggers['tb'] = tensorboardX.SummaryWriter(log_dir=self.exp_path)
-    def step_log(self, mode,step_counter,history):
-        if 'wandb' in self.loggers.keys():
-            wandb.log({
-                f"{mode}_loss": history[mode]['loss'][-1],
-            },step=step_counter[mode])
-        if 'tb' in self.loggers.keys():
-            self.loggers['tb'].add_scalar(f"{mode}_loss", history[mode]['loss'][-1], step_counter[mode])
\ No newline at end of file
+    def watch(self,model,log_freq=100):
+        wandb.watch(model, log_freq=log_freq)
+    def set_handlers(self):
+        shell_handler = RichHandler()
+        file_handler = logging.FileHandler("debug.log",mode="w")
+        # shell_handler.setLevel(logging.DEBUG)
+        file_handler.setLevel(logging.DEBUG)
+        fmt_shell = '%(message)s'
+        fmt_file = '%(levelname)s %(asctime)s [%(filename)s:%(funcName)s:%(lineno)d] %(message)s'   
+        shell_formatter = logging.Formatter(fmt_shell)
+        file_formatter = logging.Formatter(fmt_file)  
+        shell_handler.setFormatter(shell_formatter)
+        file_handler.setFormatter(file_formatter)
+        self.handlers = [shell_handler, file_handler]  
+    def info(self, msg): self.logger.info(msg)
+    def error(self, msg): self.logger.error(msg)
+    def warning(self, msg): self.logger.warning(msg)
+    def debug(self, msg): self.logger.debug(msg)
+    def exception(self, msg): self.logger.exception(msg)
+    def log(self, loss):
+        wandb.log({"loss": loss})
\ No newline at end of file
diff --git a/src/train.py b/src/train.py
index f54ebf9..131e922 100644
--- a/src/train.py
+++ b/src/train.py
@@ -51,13 +51,14 @@ def main(cfg : DictConfig) -> None:
         'seed': cfg.exp.seed if 'seed' in cfg.exp.keys() else 1,
         'resume': cfg.exp.resume if 'resume' in cfg.exp.keys() else False,
         'resume_path': cfg.exp.resume_path if 'resume_path' in cfg.exp.keys() else None,
-        'save_dir': cfg.project.save_dir if 'save_dir' in cfg.project.keys() else '.',
         'gpus': cfg.exp.gpus if 'gpus' in cfg.exp.keys() else [0],
         # additional
         'ip': cfg.exp.ip if 'ip' in cfg.exp.keys() else '127.0.0.1',
         'port': cfg.exp.port if 'port' in cfg.exp.keys() else '29500',
     }
     exp_kwargs = {
+        'project': cfg.project.name,
+        'save_dir': cfg.project.save_dir if 'save_dir' in cfg.project.keys() else '.',
         'exp_name': cfg.exp.name, 
         'trainer_kwargs': trainer_kwargs, 
         'module_kwargs': module_kwargs, 
diff --git a/src/trainer.py b/src/trainer.py
index 49a1337..b605cc5 100644
--- a/src/trainer.py
+++ b/src/trainer.py
@@ -16,12 +16,18 @@ import wandb
 import torch.distributed as dist
 import torch.multiprocessing as mp
 from torch.nn.parallel import DistributedDataParallel as DDP
-
+from logger import Logger
 class Trainer(object):
     def __init__(self, module, dataset, distributed,**kwargs):
         self.module = module
         self.dataset = dataset
         self.kwargs = copy.deepcopy(kwargs)
+        self.logger = Logger(
+            self.kwargs['project'],
+            self.kwargs['exp_name'],
+            self.kwargs['save_dir'],
+            self.kwargs
+        )
         self.trainer_kwargs = self.kwargs['trainer_kwargs']
         self.distributed = distributed
         self.seed = self.trainer_kwargs['seed'] if 'seed' in self.trainer_kwargs.keys() else random.randint(1, 10000)
@@ -29,32 +35,31 @@ class Trainer(object):
         self.epoch_history = {'train': {'loss': [], 'acc': []}, 'val': {'loss': [], 'acc': []}}
         self.events = self.kwargs['events'] if 'events' in self.kwargs.keys() else None
         # self.set_exp_path()
-    def set_exp_path(self):
-        random_str = ''.join(random.choices(string.ascii_uppercase + string.digits, k=5))
-        today = datetime.today().strftime("%Y-%m-%d")
-        curr_time = datetime.today().strftime("%H-%M")
-        exp_name = self.kwargs["exp_name"].replace(" ","_")
-        save_dir = self.trainer_kwargs['save_dir'] if self.trainer_kwargs['save_dir'][-1] != '/' else self.trainer_kwargs['save_dir'][:-1]
-        self.exp_path = f'{save_dir}/{today}/{curr_time}-{exp_name}-{random_str}'
-        # create dir if not exists
-        if not os.path.exists(save_dir): os.makedirs(save_dir)
-        if not os.path.exists(f'{save_dir}/{today}'): os.makedirs(f'{save_dir}/{today}')
-        if not os.path.exists(self.exp_path): 
-            os.makedirs(self.exp_path)
-            os.makedirs(f"{self.exp_path}/ckpts")
-            os.makedirs(f"{self.exp_path}/optims")
-            os.makedirs(f"{self.exp_path}/figs")
-            os.makedirs(f"{self.exp_path}/logs")
-        else:
-            raise Exception(f"Experiment path {self.exp_path} already exists")
-        conf = copy.deepcopy(self.kwargs)
-        conf['exp_path'] = self.exp_path
-        conf = OmegaConf.create(conf)
-        with open(f'{self.exp_path}/kwargs.yaml', 'w') as fp: 
-            OmegaConf.save(config=conf, f=fp)
+    # def set_exp_path(self):
+    #     random_str = ''.join(random.choices(string.ascii_uppercase + string.digits, k=5))
+    #     today = datetime.today().strftime("%Y-%m-%d")
+    #     curr_time = datetime.today().strftime("%H-%M")
+    #     exp_name = self.kwargs["exp_name"].replace(" ","_")
+    #     save_dir = self.trainer_kwargs['save_dir'] if self.trainer_kwargs['save_dir'][-1] != '/' else self.trainer_kwargs['save_dir'][:-1]
+    #     self.exp_path = f'{save_dir}/{today}/{curr_time}-{exp_name}-{random_str}'
+    #     # create dir if not exists
+    #     if not os.path.exists(save_dir): os.makedirs(save_dir)
+    #     if not os.path.exists(f'{save_dir}/{today}'): os.makedirs(f'{save_dir}/{today}')
+    #     if not os.path.exists(self.exp_path): 
+    #         os.makedirs(self.exp_path)
+    #         os.makedirs(f"{self.exp_path}/ckpts")
+    #         os.makedirs(f"{self.exp_path}/optims")
+    #         os.makedirs(f"{self.exp_path}/figs")
+    #         os.makedirs(f"{self.exp_path}/logs")
+    #     else:
+    #         raise Exception(f"Experiment path {self.exp_path} already exists")
+    #     conf = copy.deepcopy(self.kwargs)
+    #     conf['exp_path'] = self.exp_path
+    #     conf = OmegaConf.create(conf)
+    #     with open(f'{self.exp_path}/kwargs.yaml', 'w') as fp: 
+    #         OmegaConf.save(config=conf, f=fp)
 
     def init(self,device):
-        # wandb.init()
         torch.backends.cudnn.enabled = False
         torch.manual_seed(self.seed)
         torch.cuda.manual_seed(self.seed)
